version: '3.8'

services:

  # Single MongoDB instance (replacing sharded cluster)
  mongodb:
    image: mongo:5.0
    container_name: finance_mongodb
    hostname: mongodb
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_ROOT_USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_ROOT_PASSWORD}
      MONGO_INITDB_DATABASE: admin
    volumes:
      - mongodb_data:/data/db
    networks:
      - finance_network
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongo --quiet || exit 0
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 40s
    restart: unless-stopped

  # MongoDB setup service (simplified for single instance)
  mongo-setup:
    build:
      context: ./mongo-setup
    container_name: finance_mongo_setup
    hostname: mongo-setup
    networks:
      - finance_network
    environment:
      MONGO_ROOT_USERNAME: ${MONGO_ROOT_USERNAME}
      MONGO_ROOT_PASSWORD: ${MONGO_ROOT_PASSWORD}
      MONGODB_EXTRA_FLAGS: --quiet
      MONGO_INITDB_DATABASE: admin
    depends_on:
      mongodb:
        condition: service_healthy
    restart: "no"
    command: python3 /scripts/mongo_init.py
  # HDFS NameNode
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: finance_hdfs_namenode
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=finance_hadoop
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_replication=3
      - HDFS_CONF_dfs_permissions=false
    ports:
      - "9870:9870"
      - "8020:8020"
    networks:
      - finance_network
    restart: unless-stopped

  # HDFS DataNode 1
  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: finance_hdfs_datanode1
    volumes:
      - hadoop_datanode1:/hadoop/dfs/data
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_dfs_datanode_data_dir=/hadoop/dfs/data
      - SERVICE_PRECONDITION=namenode:9870
    depends_on:
      - namenode
    networks:
      - finance_network
    restart: unless-stopped

  # HDFS DataNode 2
  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: finance_hdfs_datanode2
    volumes:
      - hadoop_datanode2:/hadoop/dfs/data
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_dfs_datanode_data_dir=/hadoop/dfs/data
      - SERVICE_PRECONDITION=namenode:9870
    depends_on:
      - namenode
    networks:
      - finance_network
    restart: unless-stopped

  # HDFS DataNode 3
  datanode3:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: finance_hdfs_datanode3
    volumes:
      - hadoop_datanode3:/hadoop/dfs/data
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_dfs_datanode_data_dir=/hadoop/dfs/data
      - SERVICE_PRECONDITION=namenode:9870
    depends_on:
      - namenode
    networks:
      - finance_network
    restart: unless-stopped

  # Zookeeper Ensemble
  zookeeper1:
    image: confluentinc/cp-zookeeper:7.2.0
    hostname: zookeeper1
    container_name: finance_zookeeper1
    ports:
      - '2181:2181'
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_SERVERS: zookeeper1:2888:3888
    networks:
      - finance_network
    restart: on-failure
    healthcheck:
      test: ["CMD-SHELL", "echo stat | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  # Kafka Broker 1
  kafka-broker1:
    image: confluentinc/cp-kafka:7.2.0
    hostname: kafka-broker1
    container_name: finance_kafka_broker1
    depends_on:
      zookeeper1:
        condition: service_healthy
    ports:
      - '9092:9092'
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper1:2181"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker1:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks:
      - finance_network
    restart: unless-stopped

  # Spark Master
  spark-master:
    build:
      context: ./docker
      dockerfile: spark.Dockerfile
    container_name: finance_spark_master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_CONF_DIR=/spark/conf
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./spark-conf:/spark/conf
    networks:
      - finance_network
    restart: unless-stopped

  # Spark Worker 1
  spark-worker1:
    build:
      context: ./docker
      dockerfile: spark.Dockerfile
    container_name: finance_spark_worker1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_CONF_DIR=/spark/conf
    ports:
      - "8081:8081"
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./spark-conf:/spark/conf
    networks:
      - finance_network
    depends_on:
      - spark-master
    restart: unless-stopped

  # Spark Worker 2
  spark-worker2:
    build:
      context: ./docker
      dockerfile: spark.Dockerfile
    container_name: finance_spark_worker2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_WEBUI_PORT=8082
      - SPARK_CONF_DIR=/spark/conf
    ports:
      - "8082:8081"
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./spark-conf:/spark/conf
    networks:
      - finance_network
    depends_on:
      - spark-master
    restart: unless-stopped

  # Elasticsearch Cluster
  elasticsearch-master:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.3.3
    container_name: finance_elasticsearch_master
    environment:
      - node.name=master
      - cluster.name=finance-es-cluster
      - discovery.seed_hosts=elasticsearch-data1,elasticsearch-data2,elasticsearch-data3
      - cluster.initial_master_nodes=master
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=true
      - ELASTIC_PASSWORD=${ELASTICSEARCH_PASSWORD}
      - bootstrap.memory_lock=true
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - es_master_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
      - "9300:9300"
    networks:
      - finance_network
    restart: unless-stopped

  elasticsearch-data1:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.3.3
    container_name: finance_elasticsearch_data1
    environment:
      - node.name=data1
      - cluster.name=finance-es-cluster
      - discovery.seed_hosts=elasticsearch-master,elasticsearch-data2,elasticsearch-data3
      - cluster.initial_master_nodes=master
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=true
      - ELASTIC_PASSWORD=${ELASTICSEARCH_PASSWORD}
      - bootstrap.memory_lock=true
      - node.roles=["data", "ingest"]
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - es_data1_data:/usr/share/elasticsearch/data
    networks:
      - finance_network
    depends_on:
      - elasticsearch-master
    restart: unless-stopped

  elasticsearch-data2:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.3.3
    container_name: finance_elasticsearch_data2
    environment:
      - node.name=data2
      - cluster.name=finance-es-cluster
      - discovery.seed_hosts=elasticsearch-master,elasticsearch-data1,elasticsearch-data3
      - cluster.initial_master_nodes=master
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=true
      - ELASTIC_PASSWORD=${ELASTICSEARCH_PASSWORD}
      - bootstrap.memory_lock=true
      - node.roles=["data", "ingest"]
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - es_data2_data:/usr/share/elasticsearch/data
    networks:
      - finance_network
    depends_on:
      - elasticsearch-master
    restart: unless-stopped

  elasticsearch-data3:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.3.3
    container_name: finance_elasticsearch_data3
    environment:
      - node.name=data3
      - cluster.name=finance-es-cluster
      - discovery.seed_hosts=elasticsearch-master,elasticsearch-data1,elasticsearch-data2
      - cluster.initial_master_nodes=master
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=true
      - ELASTIC_PASSWORD=${ELASTICSEARCH_PASSWORD}
      - bootstrap.memory_lock=true
      - node.roles=["data", "ingest"]
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - es_data3_data:/usr/share/elasticsearch/data
    networks:
      - finance_network
    depends_on:
      - elasticsearch-master
    restart: unless-stopped

  # Kibana
  kibana:
    image: docker.elastic.co/kibana/kibana:8.3.3
    container_name: finance_kibana
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch-master:9200
      - ELASTICSEARCH_USERNAME=elastic
      - ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD}
    ports:
      - "${KIBANA_PORT}:5601"
    networks:
      - finance_network
    depends_on:
      - elasticsearch-master
    restart: unless-stopped

  # Crawler Service
  crawler:
    build:
      context: ./crawler
      dockerfile: Dockerfile
    container_name: finance_crawler
    volumes:
      - ./logs:/app/logs
      - ./configs:/app/configs
    env_file:
      - .env
    networks:
      - finance_network
    depends_on:
      - mongodb
      - kafka-broker1

  # Kafka Consumer Service
  kafka-consumer:
    build:
      context: ./kafka-consumer
      dockerfile: Dockerfile
    container_name: finance_kafka_consumer
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    env_file:
      - .env
    networks:
      - finance_network
    depends_on:
      - kafka-broker1
      - mongodb

  # ETL Service
  etl:
    build:
      context: ./spark-job
      dockerfile: Dockerfile
    container_name: finance_etl
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./configs:/app/configs
    env_file:
      - .env
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - HADOOP_CONF_DIR=/etc/hadoop/conf
      - YARN_CONF_DIR=/etc/hadoop/conf
    networks:
      - finance_network
    depends_on:
      - spark-master
      - spark-worker1
      - spark-worker2
      - namenode
      - datanode1
      - datanode2
      - datanode3
      - mongodb

  # Model Training Service
  trainer:
    build:
      context: ./trainer
      dockerfile: Dockerfile
    container_name: finance_trainer
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./configs:/app/configs
    env_file:
      - .env
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - HADOOP_CONF_DIR=/etc/hadoop/conf
      - YARN_CONF_DIR=/etc/hadoop/conf
      - TF_CONFIG=${TF_CONFIG:-'{"cluster":{"worker":["trainer:2222","spark-worker1:2223","spark-worker2:2224"]},"task":{"type":"worker","index":0}}'}
    networks:
      - finance_network
    depends_on:
      - spark-master
      - spark-worker1
      - spark-worker2
      - namenode
      - datanode1
      - datanode2
      - datanode3

  # Model Inference Service
  inference:
    build:
      context: ./inference
      dockerfile: Dockerfile
    container_name: finance_inference
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./configs:/app/configs
    env_file:
      - .env
    environment:
      - HADOOP_CONF_DIR=/etc/hadoop/conf
      - YARN_CONF_DIR=/etc/hadoop/conf
    networks:
      - finance_network
    depends_on:
      - elasticsearch-master
      - mongodb
      - namenode

  # Scheduler Service
  scheduler:
    build:
      context: ./docker
      dockerfile: scheduler.Dockerfile
    container_name: finance_scheduler
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./data:/app/data
      - ./logs:/app/logs
      - ./configs:/app/configs
    env_file:
      - .env
    networks:
      - finance_network
    depends_on:
      - mongodb
      - spark-master
      - elasticsearch-master
      - namenode
    restart: unless-stopped

networks:
  finance_network:
    name: ${NETWORK_NAME}

volumes:
  # MongoDB volume (single instance)
  mongodb_data:
  
  # HDFS volumes
  hadoop_namenode:
  hadoop_datanode1:
  hadoop_datanode2:
  hadoop_datanode3:
  
  # Elasticsearch volumes
  es_master_data:
  es_data1_data:
  es_data2_data:
  es_data3_data:
